{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWPb4gTLAAI4d3+1/ZVncv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dan1lk/data_generator/blob/main/data_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFPlauHhYU_S",
        "outputId": "89557979-4dea-4bc2-f39c-b32777ed8b84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=3d12f2de564ba5cc30d21af12bfc94491137ed58ee72ec434269e793fb0f18b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from random import choice, randint, random\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from math import floor\n",
        "import os\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Generate synthetic data\") \\\n",
        "    .config('spark.master', 'local[*]') \\\n",
        "    .config('spark.driver.memory', '4g') \\\n",
        "    .config('spark.executor.cores', '4') \\\n",
        "    .config('spark.executor.memory', '8g') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "email_example = ['yandex.ru', \"mail.ru\", \"gmail.ru\", \"inbox.ru\", \"ok.ru\", \"rambler.ru\", \"yahoo.com\", \"usa.com\", \"citeweb.com\", \"deneg.com\", \"england.com\", \"hotmail.com\", \"mail.com\", \"jetterbox.com\"]\n",
        "salary_example = [i for i in range(100000, 500000, 10000)]\n",
        "\n",
        "\n",
        "#Заполняем лист именами из файла\n",
        "names = []\n",
        "with open(\"russian_trans_names.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for i in file.readlines():\n",
        "        i_strip = i.strip(\"\\n\")\n",
        "        if len(i_strip) >= 5:\n",
        "            names.append(i.strip(\"\\n\").capitalize())\n",
        "\n",
        "#Заполняем лист городами из файла\n",
        "cities = []\n",
        "with open(\"russian-cities.json\", 'r', encoding='utf-8') as file:\n",
        "    cities_json = json.load(file)\n",
        "    for i in cities_json:\n",
        "        if len(i[\"name\"]) >= 7:\n",
        "            cities.append(i[\"name\"])\n",
        "\n",
        "#Заполняем лист email\n",
        "email = []\n",
        "for i in names:\n",
        "    email.append(f\"{i.lower()}@{choice(email_example)}\")\n",
        "\n",
        "\n",
        "\n",
        "#функция выбирает рандомную дату регистрации, учитывая что дата регистрации не может быть старше возраста челловека\n",
        "str_count = int(input('Введите количество строк: '))\n",
        "\n",
        "def random_data_registry(age):\n",
        "    years_ago = floor(random() * age)\n",
        "    data_now = datetime.now().date()\n",
        "    data_registry = data_now - timedelta(weeks=years_ago * 52) - timedelta(days=years_ago * 1)\n",
        "    return data_registry.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#Заполняем лист синтетическими данными на str_count элементов\n",
        "data = []\n",
        "for i in range(1, str_count + 1):\n",
        "\n",
        "    ch_name = choice(names)\n",
        "    ch_city = choice(cities)\n",
        "    ch_email =email[names.index(ch_name)]\n",
        "    age = randint(18, 95)\n",
        "    salary = choice(salary_example)\n",
        "    data_registry = random_data_registry(age)\n",
        "\n",
        "    #создаем список данных\n",
        "    str_data = [i, ch_name, ch_email, ch_city, age, salary, data_registry]\n",
        "    #добавляем в data\n",
        "    data.append(str_data)\n",
        "\n",
        "#Меняем в столбце значение на None, если random возвращает число < 0.05 (в 5% случаев)\n",
        "for i in range(len(data)):\n",
        "  for j in range(len(data[i])):\n",
        "    if random() < 0.05:\n",
        "      data[i][j] = None\n",
        "\n",
        "#создаем датафрейм\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"email\", \"city\", \"age\", \"salary\", \"registration_date\"])\n",
        "#путь сохранения файла csv\n",
        "path = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-dev.csv\"\n",
        "#обьединяем партиции в один файл и записываем на диск\n",
        "df.coalesce(1).write.csv(path, header=True, mode='overwrite')\n",
        "\n",
        "#переименование файла\n",
        "\n",
        "#находим название старого файла\n",
        "old_file_name = [i for i in os.listdir(path) if i.endswith(\".csv\")][0]\n",
        "#путь к файлу со старым названием\n",
        "old_file_path = os.path.join(path, old_file_name)\n",
        "#имя для нового файла\n",
        "new_file_name = path\n",
        "#путь для нового файла\n",
        "new_file_path = os.path.join(path, new_file_name)\n",
        "#меняем название файла на новое\n",
        "os.rename(old_file_path, new_file_path)\n",
        "\n",
        "#путь к success файлу\n",
        "success_file_path = os.path.join(path, '_SUCCESS')\n",
        "#удаляем этот файл из папки\n",
        "os.remove(success_file_path)\n",
        "\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "kUSlBDEJspMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc9508fc-dee5-4557-94e3-042ad6abe080"
      },
      "execution_count": 34,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите количество строк: 1000\n"
          ]
        }
      ]
    }
  ]
}